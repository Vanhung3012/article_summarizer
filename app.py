import streamlit as st
import google.generativeai as genai
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse
import time
import os
from tenacity import retry, stop_after_attempt, wait_exponential

def check_api_key():
    """
    Ki·ªÉm tra API key c√≥ t·ªìn t·∫°i v√† h·ª£p l·ªá kh√¥ng
    """
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
        if not api_key:
            st.error("‚ö†Ô∏è GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh!")
            st.stop()
        return api_key
    except Exception as e:
        st.error("‚ö†Ô∏è GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong Streamlit Secrets!")
        st.stop()

def validate_url(url):
    """
    Ki·ªÉm tra URL c√≥ h·ª£p l·ªá kh√¥ng
    """
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

class ArticleSummarizer:
    def __init__(self):
        self.gemini_api_key = check_api_key()
        genai.configure(api_key=self.gemini_api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    async def fetch_url(self, url):
        """
        ƒê·ªçc URL b·∫•t ƒë·ªìng b·ªô s·ª≠ d·ª•ng aiohttp
        """
        try:
            async with aiohttp.ClientSession(headers=self.headers) as session:
                async with session.get(url) as response:
                    return await response.text()
        except Exception as e:
            raise Exception(f"L·ªói khi ƒë·ªçc URL {url}: {str(e)}")

    def extract_content_from_html(self, html):
        """
        Tr√≠ch xu·∫•t n·ªôi dung t·ª´ HTML s·ª≠ d·ª•ng BeautifulSoup
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'iframe']):
                tag.decompose()
            
            # L·∫•y n·ªôi dung t·ª´ c√°c th·∫ª p
            paragraphs = soup.find_all('p')
            content = ' '.join([p.get_text().strip() for p in paragraphs])
            
            return content
        except Exception as e:
            raise Exception(f"L·ªói khi parse HTML: {str(e)}")

    async def extract_content_from_url(self, url):
        """
        Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL
        """
        html = await self.fetch_url(url)
        return self.extract_content_from_html(html)

    async def process_urls(self, urls):
        """
        X·ª≠ l√Ω nhi·ªÅu URLs ƒë·ªìng th·ªùi
        """
        try:
            start_time = time.time()
            
            # ƒê·ªçc n·ªôi dung t·ª´ t·∫•t c·∫£ URLs ƒë·ªìng th·ªùi
            contents = await asyncio.gather(
                *[self.extract_content_from_url(url.strip()) for url in urls]
            )
            
            # K·∫øt h·ª£p n·ªôi dung
            combined_content = "\n\n---\n\n".join(contents)
            
            print(f"Th·ªùi gian ƒë·ªçc URLs: {time.time() - start_time:.2f} gi√¢y")
            
            # X·ª≠ l√Ω v·ªõi Gemini
            result = await self.process_content(combined_content, urls)
            
            print(f"T·ªïng th·ªùi gian x·ª≠ l√Ω: {time.time() - start_time:.2f} gi√¢y")
            
            return result
            
        except Exception as e:
            raise Exception(f"L·ªói x·ª≠ l√Ω URLs: {str(e)}")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        reraise=True
    )
    async def call_gemini_api(self, prompt):
        """
        G·ªçi Gemini API v·ªõi retry v√† rate limit
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            if "429" in str(e):
                st.warning("ƒêang ch·ªù API... Vui l√≤ng ƒë·ª£i trong gi√¢y l√°t.")
                time.sleep(5)
                raise e
            raise e

    async def process_content(self, content, urls):
        """
        X·ª≠ l√Ω n·ªôi dung v·ªõi Gemini
        """
        try:
            # B∆∞·ªõc 1: T√≥m t·∫Øt 3 b√†i b√°o th√†nh m·ªôt b√†i b√°o ti·∫øng Anh h∆°n 500 ch·ªØ
            english_prompt = f"""
            Create a comprehensive article summarizing the following three articles into one English article with more than 500 words.

            Text to process: {content[:15000]}
            """
            
            english_result = await self.call_gemini_api(english_prompt)
            
            # B∆∞·ªõc 2: D·ªãch sang ti·∫øng Vi·ªát v√† ƒë·∫∑t ti√™u ƒë·ªÅ thu h√∫t d∆∞·ªõi 15 t·ª´
            vietnamese_prompt = f"""
            Translate this English article to Vietnamese and create a compelling title under 15 words.

            English text:
            {english_result}
            """
            
            vietnamese_result = await self.call_gemini_api(vietnamese_prompt)
            
            # Ghi l·∫°i k·∫øt qu·∫£ ƒë·ªÉ ki·ªÉm tra
            print("K·∫øt qu·∫£ t·ª´ Gemini:", vietnamese_result)  # Ghi l·∫°i k·∫øt qu·∫£ ƒë·ªÉ ki·ªÉm tra
            
            # Ph√¢n t√≠ch k·∫øt qu·∫£ ti·∫øng Vi·ªát
            try:
                if 'TITLE:' in vietnamese_result and 'SUMMARY:' in vietnamese_result:
                    vi_title = vietnamese_result.split('TITLE:')[1].split('SUMMARY:')[0].strip()
                    vi_summary = vietnamese_result.split('SUMMARY:')[1].strip()
                    
                    return {
                        'title': vi_title,
                        'content': vi_summary,
                        'original_urls': urls
                    }
                else:
                    raise Exception("K·∫øt qu·∫£ kh√¥ng ch·ª©a TITLE ho·∫∑c SUMMARY.")
                
            except Exception as e:
                raise Exception(f"Kh√¥ng th·ªÉ parse k·∫øt qu·∫£ ti·∫øng Vi·ªát: {str(e)}")
            
        except Exception as e:
            raise Exception(f"L·ªói x·ª≠ l√Ω Gemini: {str(e)}")

    async def refine_summary(self, summary):
        """
        Ch·ªânh s·ª≠a n·ªôi dung t√≥m t·∫Øt ƒë·ªÉ gi·ªëng m·ªôt b√†i b√°o h∆°n
        """
        prompt = f"""
        Please refine the following summary to make it sound more like a professional article. 
        Ensure that the language is formal, coherent, and engaging.
        Do not include any headings, subheadings, or bullet points.

        Current summary:
        {summary}
        """
        refined_summary = await self.call_gemini_api(prompt)
        return refined_summary

async def process_and_update_ui(summarizer, urls):
    try:
        result = await summarizer.process_urls(urls)
        return result
    except Exception as e:
        raise e

def main():
    st.set_page_config(page_title="·ª®ng d·ª•ng T√≥m t·∫Øt VƒÉn b·∫£n", page_icon="üìù", layout="wide")
    
    st.title("üìù ·ª®ng d·ª•ng T√≥m t·∫Øt Nhi·ªÅu B√†i B√°o")
    st.markdown("---")

    if 'summarizer' not in st.session_state:
        st.session_state.summarizer = ArticleSummarizer()

    with st.container():
        st.subheader("üîó Nh·∫≠p URL c√°c b√†i b√°o")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            url1 = st.text_input("URL b√†i b√°o 1", key="url1")
        with col2:
            url2 = st.text_input("URL b√†i b√°o 2", key="url2")
        with col3:
            url3 = st.text_input("URL b√†i b√°o 3", key="url3")
        
        urls = [url1, url2, url3]
        
        if st.button("T√≥m t·∫Øt", type="primary"):
            if not all(urls):
                st.warning("Vui l√≤ng nh·∫≠p ƒë·ªß 3 URLs!")
                return
            
            invalid_urls = [url for url in urls if not validate_url(url)]
            if invalid_urls:
                st.error(f"C√°c URLs sau kh√¥ng h·ª£p l·ªá: {', '.join(invalid_urls)}")
                return
            
            progress_text = "ƒêang x·ª≠ l√Ω..."
            progress_bar = st.progress(0, text=progress_text)
            
            try:
                result = asyncio.run(process_and_update_ui(st.session_state.summarizer, urls))
                
                if result:
                    progress_bar.progress(100, text="Ho√†n th√†nh!")
                    st.success(f"‚úÖ T√≥m t·∫Øt th√†nh c√¥ng! (ƒê·ªô d√†i: {result['vi_word_count']} t·ª´ ti·∫øng Vi·ªát, {result['word_count']} t·ª´ ti·∫øng Anh)")
                    
                    st.markdown(f"## üìå {result['title']}")
                    st.markdown("### üìÑ B·∫£n t√≥m t·∫Øt")
                    st.write(result['content'])
                    
                    with st.expander("Xem phi√™n b·∫£n ti·∫øng Anh"):
                        st.markdown(f"### {result['english_title']}")
                        st.write(result['english_summary'])
                    
                    with st.expander("Xem URLs g·ªëc"):
                        for i, url in enumerate(result['original_urls'], 1):
                            st.write(f"B√†i {i}: [{url}]({url})", unsafe_allow_html=True)
                            
            except Exception as e:
                st.error(f"C√≥ l·ªói x·∫£y ra: {str(e)}")
            finally:
                progress_bar.empty()

if __name__ == "__main__":
    main()

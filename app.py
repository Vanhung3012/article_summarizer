import streamlit as st
import google.generativeai as genai
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import time
from tenacity import retry, stop_after_attempt, wait_exponential

def check_api_key():
    """
    Kiá»ƒm tra API key Gemini
    """
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
        if not api_key:
            st.error("âš ï¸ Vui lÃ²ng cáº¥u hÃ¬nh GEMINI_API_KEY!")
            st.stop()
        return api_key
    except Exception as e:
        st.error("âš ï¸ GEMINI_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh trong Streamlit Secrets!")
        st.stop()

def validate_url(url):
    """
    Kiá»ƒm tra URL há»£p lá»‡
    """
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

class NewsArticleGenerator:
    def __init__(self):
        self.gemini_api_key = check_api_key()
        genai.configure(api_key=self.gemini_api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    async def fetch_url(self, url):
        """
        Äá»c ná»™i dung tá»« URL
        """
        try:
            async with aiohttp.ClientSession(headers=self.headers) as session:
                async with session.get(url) as response:
                    return await response.text()
        except Exception as e:
            raise Exception(f"Lá»—i khi Ä‘á»c URL {url}: {str(e)}")

    def extract_content(self, html):
        """
        TrÃ­ch xuáº¥t ná»™i dung tá»« HTML
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Loáº¡i bá» cÃ¡c pháº§n khÃ´ng cáº§n thiáº¿t
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'iframe', 'aside']):
                tag.decompose()
            
            # Láº¥y tiÃªu Ä‘á»
            title = ""
            if soup.find('h1'):
                title = soup.find('h1').get_text().strip()
            elif soup.find('title'):
                title = soup.find('title').get_text().strip()
            
            # Láº¥y ná»™i dung chÃ­nh
            article_tags = soup.find_all(['article', 'main', 'div'], class_=['content', 'article', 'post'])
            content = ""
            
            if article_tags:
                for tag in article_tags:
                    paragraphs = tag.find_all('p')
                    content += ' '.join([p.get_text().strip() for p in paragraphs])
            else:
                # Náº¿u khÃ´ng tÃ¬m tháº¥y tháº» article, láº¥y táº¥t cáº£ tháº» p
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs])
            
            return {
                'title': title,
                'content': content
            }
            
        except Exception as e:
            raise Exception(f"Lá»—i khi xá»­ lÃ½ HTML: {str(e)}")

    async def scrape_articles(self, urls):
        """
        Thu tháº­p ná»™i dung tá»« nhiá»u URLs
        """
        articles = []
        for url in urls:
            if url.strip():
                html = await self.fetch_url(url)
                content = self.extract_content(html)
                articles.append({
                    'url': url,
                    'title': content['title'],
                    'content': content['content']
                })
        return articles

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        reraise=True
    )
    async def call_gemini_api(self, prompt):
        """
        Gá»i Gemini API vá»›i retry
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            if "429" in str(e):
                st.warning("Äang chá» API... Vui lÃ²ng Ä‘á»£i trong giÃ¢y lÃ¡t")
                time.sleep(5)
                raise e
            raise e

    async def generate_article(self, articles):
        """
        Táº¡o bÃ i bÃ¡o tá»« nhiá»u nguá»“n
        """
        try:
            # Tá»•ng há»£p ná»™i dung tá»« cÃ¡c bÃ i bÃ¡o
            combined_content = "\n\n---\n\n".join(
                [f"TiÃªu Ä‘á»: {a['title']}\nNá»™i dung: {a['content']}" for a in articles]
            )

            # Prompt Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  tá»•ng há»£p thÃ nh bÃ i bÃ¡o má»›i
            analysis_prompt = f"""
            PhÃ¢n tÃ­ch vÃ  tá»•ng há»£p thÃ nh má»™t bÃ i bÃ¡o má»›i tá»« cÃ¡c nguá»“n sau:

            {combined_content}

            YÃªu cáº§u:

            1. TiÃªu Ä‘á» bÃ i bÃ¡o:
               - Tá»‘i Ä‘a 15 tá»«
               - Thu hÃºt, táº¡o áº¥n tÆ°á»£ng máº¡nh
               - Pháº£n Ã¡nh chÃ­nh xÃ¡c ná»™i dung chÃ­nh
               - Sá»­ dá»¥ng tá»« ngá»¯ bÃ¡o chÃ­ chuáº©n má»±c
               - TrÃ¡nh giáº­t gÃ¢n, cÃ¢u view

            2. Cáº¥u trÃºc bÃ i viáº¿t:
               - TÃ³m táº¯t Ã½ chÃ­nh trong Ä‘oáº¡n má»Ÿ Ä‘áº§u (3-4 cÃ¢u)
               - Triá»ƒn khai chi tiáº¿t theo logic rÃµ rÃ ng
               - Dáº«n nguá»“n vÃ  trÃ­ch dáº«n khi cáº§n
               - PhÃ¢n tÃ­ch, Ä‘Ã¡nh giÃ¡ khÃ¡ch quan
               - Káº¿t luáº­n sÃºc tÃ­ch, Ä‘áº§y Ä‘á»§

            3. Ná»™i dung:
               - Tá»•ng há»£p thÃ´ng tin tá»« nhiá»u nguá»“n
               - Äáº£m báº£o tÃ­nh chÃ­nh xÃ¡c
               - Cung cáº¥p gÃ³c nhÃ¬n Ä‘a chiá»u
               - ThÃªm sá»‘ liá»‡u, dá»¯ liá»‡u cá»¥ thá»ƒ
               - Äá»™ dÃ i 800-1000 tá»«

            4. NgÃ´n ngá»¯:
               - Trong sÃ¡ng, dá»… hiá»ƒu
               - Phong cÃ¡ch bÃ¡o chÃ­ chuyÃªn nghiá»‡p
               - KhÃ¡ch quan, trung láº­p
               - TrÃ¡nh tá»« ngá»¯ cáº£m xÃºc, thiÃªn kiáº¿n
               - Chá»n lá»c tá»« ngá»¯ phÃ¹ há»£p vÄƒn phong

            Format pháº£n há»“i:
            TITLE: [tiÃªu Ä‘á» bÃ i bÃ¡o]
            ARTICLE: [ná»™i dung bÃ i bÃ¡o]
            """

            # Gá»i API Ä‘á»ƒ táº¡o bÃ i bÃ¡o
            result = await self.call_gemini_api(analysis_prompt)
            
            try:
                title = result.split('TITLE:')[1].split('ARTICLE:')[0].strip()
                content = result.split('ARTICLE:')[1].strip()
                
                # Kiá»ƒm tra Ä‘á»™ dÃ i tiÃªu Ä‘á»
                if len(title.split()) > 15:
                    optimize_title_prompt = f"""
                    Tá»‘i Æ°u tiÃªu Ä‘á» sau Ä‘á»ƒ ngáº¯n gá»n hÆ¡n (tá»‘i Ä‘a 15 tá»«) nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c Ã½ chÃ­nh:
                    {title}

                    YÃªu cáº§u:
                    - RÃºt gá»n nhÆ°ng khÃ´ng máº¥t Ã½ nghÄ©a
                    - Váº«n pháº£i thu hÃºt, áº¥n tÆ°á»£ng
                    - DÃ¹ng tá»« ngá»¯ chÃ­nh xÃ¡c, sÃºc tÃ­ch
                    - PhÃ¹ há»£p phong cÃ¡ch bÃ¡o chÃ­

                    Format: TITLE: [tiÃªu Ä‘á» tá»‘i Æ°u]
                    """
                    title_result = await self.call_gemini_api(optimize_title_prompt)
                    title = title_result.split('TITLE:')[1].strip()
                
                # Kiá»ƒm tra Ä‘á»™ dÃ i ná»™i dung
                word_count = len(content.split())
                if word_count < 800:
                    expand_prompt = f"""
                    Má»Ÿ rá»™ng ná»™i dung bÃ i bÃ¡o sau Ä‘á»ƒ Ä‘áº¡t 800-1000 tá»«.
                    ThÃªm chi tiáº¿t, phÃ¢n tÃ­ch sÃ¢u hÆ¡n nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c tÃ­nh máº¡ch láº¡c vÃ  phong cÃ¡ch ban Ä‘áº§u.

                    BÃ i bÃ¡o hiá»‡n táº¡i:
                    {content}
                    """
                    content = await self.call_gemini_api(expand_prompt)
                
                return {
                    'title': title,
                    'content': content,
                    'word_count': len(content.split()),
                    'sources': [a['url'] for a in articles]
                }
                
            except Exception as e:
                raise Exception(f"Lá»—i khi xá»­ lÃ½ káº¿t quáº£: {str(e)}")
            
        except Exception as e:
            raise Exception(f"Lá»—i khi táº¡o bÃ i bÃ¡o: {str(e)}")

def main():
    st.set_page_config(
        page_title="Tá»•ng Há»£p Tin Tá»©c", 
        page_icon="ğŸ“°",
        layout="wide"
    )
    
    st.title("ğŸ“° á»¨ng Dá»¥ng Tá»•ng Há»£p Tin Tá»©c")
    st.markdown("""
    á»¨ng dá»¥ng nÃ y giÃºp tá»•ng há»£p vÃ  viáº¿t láº¡i ná»™i dung tá»« nhiá»u bÃ i bÃ¡o thÃ nh má»™t bÃ i bÃ¡o má»›i, 
    Ä‘áº£m báº£o tÃ­nh chuyÃªn nghiá»‡p vÃ  cháº¥t lÆ°á»£ng.
    """)
    st.markdown("---")

    if 'generator' not in st.session_state:
        st.session_state.generator = NewsArticleGenerator()

    with st.container():
        st.subheader("ğŸ”— Nháº­p URLs BÃ i BÃ¡o")
        
        # Táº¡o 3 cá»™t Ä‘á»ƒ nháº­p URL
        cols = st.columns(3)
        urls = []
        for i, col in enumerate(cols, 1):
            with col:
                url = st.text_input(
                    f"URL bÃ i bÃ¡o {i}",
                    key=f"url{i}",
                    placeholder="https://..."
                )
                urls.append(url)
        
        # NÃºt táº¡o bÃ i bÃ¡o
        if st.button("Táº¡o BÃ i BÃ¡o", type="primary"):
            # Kiá»ƒm tra URLs
            valid_urls = [url for url in urls if url.strip()]
            if len(valid_urls) == 0:
                st.warning("âš ï¸ Vui lÃ²ng nháº­p Ã­t nháº¥t má»™t URL!")
                return
                
            invalid_urls = [url for url in valid_urls if not validate_url(url)]
            if invalid_urls:
                st.error(f"âŒ URL khÃ´ng há»£p lá»‡: {', '.join(invalid_urls)}")
                return
            
            # Hiá»ƒn thá»‹ thanh tiáº¿n trÃ¬nh
            progress = st.progress(0)
            status = st.empty()
            
            try:
                with st.spinner("Äang xá»­ lÃ½..."):
                    # Thu tháº­p ná»™i dung
                    status.text("Äang Ä‘á»c ná»™i dung tá»« cÃ¡c URLs...")
                    progress.progress(25)
                    
                    articles = asyncio.run(
                        st.session_state.generator.scrape_articles(valid_urls)
                    )
                    
                    if not articles:
                        st.error("âŒ KhÃ´ng thá»ƒ Ä‘á»c ná»™i dung tá»« cÃ¡c URLs!")
                        return
                    
                    # Táº¡o bÃ i bÃ¡o
                    status.text("Äang tá»•ng há»£p vÃ  viáº¿t bÃ i...")
                    progress.progress(50)
                    
                    result = asyncio.run(
                        st.session_state.generator.generate_article(articles)
                    )
                    
                    if result:
                        progress.progress(100)
                        status.empty()
                        
                        # Hiá»ƒn thá»‹ káº¿t quáº£
                        st.success(f"âœ… ÄÃ£ táº¡o bÃ i bÃ¡o thÃ nh cÃ´ng! ({result['word_count']} tá»«)")
                        
                        st.markdown(f"## ğŸ“Œ {result['title']}")
                        st.markdown("### ğŸ“„ Ná»™i dung")
                        st.write(result['content'])
                        
                        with st.expander("ğŸ” Xem nguá»“n bÃ i viáº¿t"):
                            for i, url in enumerate(result['sources'], 1):
                                st.write(f"{i}. [{url}]({url})")
                        
            except Exception as e:
                st.error(f"âŒ CÃ³ lá»—i xáº£y ra: {str(e)}")
            finally:
                progress.empty()
                status.empty()

if __name__ == "__main__":
    main()

import streamlit as st
import google.generativeai as genai
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import time
from tenacity import retry, stop_after_attempt, wait_exponential
import base64
from io import BytesIO
from PIL import Image

def check_api_key():
    """
    Ki·ªÉm tra API key Gemini
    """
    try:
        api_key = st.secrets.get("GEMINI_API_KEY")
        if not api_key:
            st.error("‚ö†Ô∏è Vui l√≤ng c·∫•u h√¨nh GEMINI_API_KEY trong Streamlit Secrets!")
            st.stop()
        return api_key
    except Exception as e:
        st.error(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc GEMINI_API_KEY: {str(e)}")
        st.stop()

def validate_url(url):
    """
    Ki·ªÉm tra URL h·ª£p l·ªá
    """
    try:
        result = urlparse(url.strip())
        return all([result.scheme in ['http', 'https'], result.netloc])
    except:
        return False

class NewsArticleGenerator:
    def __init__(self):
        self.gemini_api_key = check_api_key()
        genai.configure(api_key=self.gemini_api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi,en-US;q=0.9,en;q=0.8',
            'Connection': 'keep-alive'
        }
        self.session = None

    async def get_session(self):
        """
        T·∫°o v√† t√°i s·ª≠ d·ª•ng session
        """
        if self.session is None:
            self.session = aiohttp.ClientSession(headers=self.headers)
        return self.session

    async def fetch_url(self, url):
        """
        ƒê·ªçc n·ªôi dung t·ª´ URL v·ªõi x·ª≠ l√Ω l·ªói t·ªët h∆°n
        """
        try:
            session = await self.get_session()
            async with session.get(url.strip(), timeout=30) as response:
                if response.status != 200:
                    raise Exception(f"HTTP {response.status}")
                return await response.text()
        except asyncio.TimeoutError:
            raise Exception("Timeout khi ƒë·ªçc URL")
        except Exception as e:
            raise Exception(f"L·ªói khi ƒë·ªçc URL: {str(e)}")

    async def fetch_image(self, url):
        """
        T·∫£i ·∫£nh t·ª´ URL v·ªõi x·ª≠ l√Ω l·ªói t·ªët h∆°n
        """
        try:
            session = await self.get_session()
            async with session.get(url.strip(), timeout=20) as response:
                if response.status == 200:
                    content_type = response.headers.get('content-type', '')
                    if not content_type.startswith('image/'):
                        return None
                    return await response.read()
                return None
        except Exception as e:
            st.warning(f"‚ö†Ô∏è L·ªói khi t·∫£i h√¨nh ·∫£nh t·ª´ {url}: {str(e)}")
            return None

    def extract_content(self, html):
        """
        Tr√≠ch xu·∫•t n·ªôi dung v√† h√¨nh ·∫£nh v·ªõi c·∫£i thi·ªán
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng c·∫ßn thi·∫øt
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'iframe', 'aside', 'form', 'button']):
                tag.decompose()
            
            # L·∫•y ti√™u ƒë·ªÅ v·ªõi nhi·ªÅu pattern ph·ªï bi·∫øn
            title_tags = soup.find_all(['h1', 'meta'], attrs={'property': ['og:title', 'twitter:title']})
            title = next((tag.get('content', '') if tag.name == 'meta' else tag.get_text() 
                         for tag in title_tags if tag), '')
            if not title and soup.title:
                title = soup.title.get_text()
            
            # L·∫•y n·ªôi dung ch√≠nh v·ªõi nhi·ªÅu pattern
            content_tags = []
            for tag in soup.find_all(['article', 'main', 'div']):
                if any(c in (tag.get('class', []) + [tag.get('id', '')]) 
                       for c in ['content', 'article', 'post', 'detail', 'body']):
                    content_tags.append(tag)
            
            content = ""
            if content_tags:
                for tag in content_tags:
                    paragraphs = tag.find_all(['p', 'div'], recursive=False)
                    content += ' '.join(p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 100)
            else:
                paragraphs = soup.find_all('p')
                content = ' '.join(p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 100)

            # L·∫•y h√¨nh ·∫£nh c√≥ k√≠ch th∆∞·ªõc ph√π h·ª£p
            images = []
            for img in soup.find_all('img'):
                src = img.get('src', '') or img.get('data-src', '')
                if src and src.startswith(('http', '//')):
                    if not src.startswith('http'):
                        src = 'https:' + src
                    width = img.get('width', '0')
                    height = img.get('height', '0')
                    try:
                        w = int(width)
                        h = int(height)
                        if w < 200 or h < 200:  # B·ªè qua ·∫£nh nh·ªè
                            continue
                    except:
                        pass
                    alt = img.get('alt', '') or img.get('title', '')
                    images.append({
                        'url': src,
                        'alt': alt
                    })
            
            return {
                'title': title.strip(),
                'content': content.strip(),
                'images': images[:3]
            }
            
        except Exception as e:
            raise Exception(f"L·ªói khi x·ª≠ l√Ω HTML: {str(e)}")

    async def close(self):
        """
        ƒê√≥ng session khi k·∫øt th√∫c
        """
        if self.session:
            await self.session.close()
            self.session = None

    async def scrape_articles(self, urls):
        """
        Thu th·∫≠p n·ªôi dung t·ª´ nhi·ªÅu URLs
        """
        try:
            articles = []
            for url in urls:
                if url.strip():
                    html = await self.fetch_url(url)
                    if not html:
                        continue
                    
                    content = self.extract_content(html)
                    if not content['content']:
                        continue
                        
                    # T·∫£i c√°c h√¨nh ·∫£nh
                    images = []
                    for img in content['images']:
                        img_data = await self.fetch_image(img['url'])
                        if img_data:
                            images.append({
                                'data': img_data,
                                'alt': img['alt']
                            })
                    
                    articles.append({
                        'url': url,
                        'title': content['title'],
                        'content': content['content'],
                        'images': images
                    })
            return articles
        finally:
            await self.close()

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        reraise=True
    )
    async def call_gemini_api(self, prompt):
        """
        G·ªçi Gemini API v·ªõi retry v√† x·ª≠ l√Ω l·ªói t·ªët h∆°n
        """
        try:
            response = self.model.generate_content(prompt)
            if not response or not response.text:
                raise Exception("Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ API")
            return response.text
        except Exception as e:
            if "429" in str(e):
                st.warning("‚è≥ ƒêang ch·ªù API... Vui l√≤ng ƒë·ª£i trong gi√¢y l√°t")
                time.sleep(5)
            raise Exception(f"L·ªói API: {str(e)}")

    async def generate_article(self, articles):
        """
        T·∫°o b√†i b√°o v·ªõi prompt ƒë∆∞·ª£c c·∫£i thi·ªán
        """
        try:
            if not articles:
                raise Exception("Kh√¥ng c√≥ b√†i b√°o n√†o ƒë·ªÉ t·ªïng h·ª£p")
                
            # T·ªïng h·ª£p n·ªôi dung t·ª´ c√°c b√†i b√°o
            combined_content = "\n\n---\n\n".join(
                f"Ti√™u ƒë·ªÅ: {a['title']}\nN·ªôi dung: {a['content'][:2000]}"  # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh qu√° t·∫£i
                for a in articles
            )

            # Prompt ƒë∆∞·ª£c c·∫£i thi·ªán
            analysis_prompt = f"""
            H√£y ph√¢n t√≠ch v√† t·ªïng h·ª£p th√†nh m·ªôt b√†i b√°o m·ªõi t·ª´ c√°c ngu·ªìn sau:

            {combined_content}

            Y√™u c·∫ßu c·ª• th·ªÉ:

            1. Ti√™u ƒë·ªÅ (t·ªëi ƒëa 15 t·ª´):
               - S√∫c t√≠ch, thu h√∫t nh∆∞ng kh√¥ng gi·∫≠t g√¢n
               - Ph·∫£n √°nh ch√≠nh x√°c n·ªôi dung ch√≠nh
               - D√πng t·ª´ ng·ªØ b√°o ch√≠ chu·∫©n m·ª±c

            2. C·∫•u tr√∫c:
               - T√≥m t·∫Øt (3-4 c√¢u) n√™u √Ω ch√≠nh
               - Tri·ªÉn khai theo tr√¨nh t·ª± logic
               - D·∫´n ngu·ªìn khi tr√≠ch d·∫´n
               - Ph√¢n t√≠ch kh√°ch quan, ƒëa chi·ªÅu
               - K·∫øt lu·∫≠n ng·∫Øn g·ªçn, ƒë·∫ßy ƒë·ªß

            3. N·ªôi dung (800-1000 t·ª´):
               - T·ªïng h·ª£p v√† x√°c th·ª±c th√¥ng tin
               - C√¢n b·∫±ng c√°c g√≥c nh√¨n
               - S·ªë li·ªáu, d·ªØ li·ªáu c·ª• th·ªÉ
               - Ch√∫ th√≠ch h√¨nh ·∫£nh ph√π h·ª£p

            4. VƒÉn phong:
               - Trong s√°ng, chuy√™n nghi·ªáp
               - Kh√°ch quan, phi thi√™n ki·∫øn
               - T·ª´ ng·ªØ ch√≠nh x√°c, d·ªÖ hi·ªÉu
               - ƒê·∫£m b·∫£o t√≠nh b√°o ch√≠

            ƒê·ªãnh d·∫°ng ph·∫£n h·ªìi:
            TITLE: [ti√™u ƒë·ªÅ]

            ARTICLE: [n·ªôi dung]
            """

            # G·ªçi API ƒë·ªÉ t·∫°o b√†i b√°o
            result = await self.call_gemini_api(analysis_prompt)
            
            try:
                # X·ª≠ l√Ω k·∫øt qu·∫£
                parts = result.split('TITLE:', 1)
                if len(parts) != 2:
                    raise Exception("ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ kh√¥ng h·ª£p l·ªá")
                
                content_parts = parts[1].split('ARTICLE:', 1)
                if len(content_parts) != 2:
                    raise Exception("ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ kh√¥ng h·ª£p l·ªá")
                
                title = content_parts[0].strip()
                content = content_parts[1].strip()
                
                # Ki·ªÉm tra v√† t·ªëi ∆∞u ti√™u ƒë·ªÅ n·∫øu c·∫ßn
                if len(title.split()) > 15:
                    title_prompt = f"""
                    T·ªëi ∆∞u ti√™u ƒë·ªÅ sau (t·ªëi ƒëa 15 t·ª´):
                    {title}

                    Y√™u c·∫ßu:
                    - Ng·∫Øn g·ªçn, ƒë·∫ßy ƒë·ªß √Ω
                    - Thu h√∫t, chuy√™n nghi·ªáp
                    - T·ª´ ng·ªØ ch√≠nh x√°c

                    Tr·∫£ v·ªÅ: TITLE: [ti√™u ƒë·ªÅ m·ªõi]
                    """
                    title_result = await self.call_gemini_api(title_prompt)
                    if 'TITLE:' in title_result:
                        title = title_result.split('TITLE:')[1].strip()
                
                return {
                    'title': title,
                    'content': content,
                    'word_count': len(content.split()),
                    'sources': [a['url'] for a in articles],
                    'images': [img for a in articles for img in a['images']]
                }
                
            except Exception as e:
                raise Exception(f"L·ªói khi x·ª≠ l√Ω k·∫øt qu·∫£: {str(e)}")
            
        except Exception as e:
            raise Exception(f"L·ªói khi t·∫°o b√†i b√°o: {str(e)}")

def main():
    st.set_page_config(
        page_title="T·ªïng H·ª£p Tin T·ª©c", 
        page_icon="üì∞",
        layout="wide"
    )
    
    st.title("üì∞ ·ª®ng D·ª•ng T·ªïng H·ª£p Tin T·ª©c")
    st.markdown("""
    ### Gi·ªõi thi·ªáu
    ·ª®ng d·ª•ng n√†y gi√∫p t·ªïng h·ª£p v√† vi·∫øt l·∫°i n·ªôi dung t·ª´ nhi·ªÅu b√†i b√°o th√†nh m·ªôt b√†i b√°o m·ªõi, 
    ƒë·∫£m b·∫£o t√≠nh chuy√™n nghi·ªáp v√† ch·∫•t l∆∞·ª£ng th√¥ng qua c√¥ng ngh·ªá AI.
    
    #### C√°ch s·ª≠ d·ª•ng:
    1. Nh·∫≠p URL c·ªßa c√°c b√†i b√°o mu·ªën t·ªïng h·ª£p (t·ªëi thi·ªÉu 1 b√†i)
    2. Nh·∫•n "T·∫°o B√†i B√°o" v√† ƒë·ª£i trong gi√¢y l√°t
    3. Xem k·∫øt qu·∫£ v√† t·∫£i xu·ªëng theo ƒë·ªãnh d·∫°ng mong mu·ªën
    """)
    st.markdown("---")

    if 'generator' not in st.session_state:
        st.session_state.generator = NewsArticleGenerator()

    with st.container():
        st.subheader("üîó Nh·∫≠p URLs B√†i B√°o")
        
        # T·∫°o 3 c·ªôt ƒë·ªÉ nh·∫≠p URL
        cols = st.columns(3)
        urls = []
        for i, col in enumerate(cols, 1):
            with col:
                url = st.text_input(
                    f"URL b√†i b√°o {i}",
                    key=f"url{i}",
                    placeholder="https://..."
                )
                if url:
                    if not validate_url(url):
                        st.error(f"‚ö†Ô∏è URL {i} kh√¥ng h·ª£p l·ªá!")
                    else:
                        urls.append(url)

        if not urls:
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt URL b√†i b√°o!")
            st.stop()

        if st.button("üîÑ T·∫°o B√†i B√°o", type="primary"):
            with st.spinner("‚è≥ ƒêang t·ªïng h·ª£p n·ªôi dung..."):
                try:
                    # T·∫°o v√† hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh
                    progress_bar = st.progress(0)
                    
                    # Thu th·∫≠p n·ªôi dung t·ª´ c√°c URLs
                    articles = asyncio.run(st.session_state.generator.scrape_articles(urls))
                    progress_bar.progress(50)
                    
                    if not articles:
                        st.error("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c n·ªôi dung t·ª´ c√°c URL ƒë√£ nh·∫≠p!")
                        st.stop()
                    
                    # T·∫°o b√†i b√°o m·ªõi
                    result = asyncio.run(st.session_state.generator.generate_article(articles))
                    progress_bar.progress(100)
                    
                    # Hi·ªÉn th·ªã k·∫øt qu·∫£
                    st.success("‚úÖ ƒê√£ t·∫°o b√†i b√°o th√†nh c√¥ng!")
                    
                    # Container cho b√†i b√°o
                    with st.container():
                        st.markdown("---")
                        st.subheader("üìù B√†i B√°o ƒê√£ T·∫°o")
                        
                        # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ
                        st.markdown(f"## {result['title']}")
                        
                        # Hi·ªÉn th·ªã h√¨nh ·∫£nh n·∫øu c√≥
                        if result['images']:
                            cols = st.columns(min(3, len(result['images'])))
                            for idx, (col, img) in enumerate(zip(cols, result['images'])):
                                with col:
                                    try:
                                        image = Image.open(BytesIO(img['data']))
                                        st.image(image, caption=img['alt'] if img['alt'] else f"H√¨nh {idx + 1}")
                                    except Exception:
                                        st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ hi·ªÉn th·ªã h√¨nh ·∫£nh")
                        
                        # Hi·ªÉn th·ªã n·ªôi dung
                        st.markdown(result['content'])
                        
                        # Th√¥ng tin th√™m
                        st.markdown("---")
                        st.markdown(f"**S·ªë t·ª´:** {result['word_count']}")
                        st.markdown("**Ngu·ªìn tham kh·∫£o:**")
                        for url in result['sources']:
                            st.markdown(f"- {url}")
                        
                        # T·∫£i xu·ªëng
                        st.markdown("---")
                        st.subheader("üíæ T·∫£i xu·ªëng")
                        
                        # T·∫°o n·ªôi dung Markdown
                        markdown_content = f"""# {result['title']}\n\n{result['content']}\n\n---\n
S·ªë t·ª´: {result['word_count']}\n\nNgu·ªìn tham kh·∫£o:\n""" + "\n".join(f"- {url}" for url in result['sources'])
                        
                        # T·∫°o button t·∫£i xu·ªëng
                        markdown_bytes = markdown_content.encode()
                        b64 = base64.b64encode(markdown_bytes).decode()
                        href = f'data:text/markdown;base64,{b64}'
                        st.markdown(f'<a href="{href}" download="bao_tong_hop.md" class="button">üì• T·∫£i xu·ªëng ƒë·ªãnh d·∫°ng Markdown</a>', unsafe_allow_html=True)
                        
                except Exception as e:
                    st.error(f"‚ùå L·ªói: {str(e)}")
                    st.stop()

if __name__ == "__main__":
    main()
    

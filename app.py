import streamlit as st
import google.generativeai as genai
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse
import time
import os

def check_api_key():
    """
    Ki·ªÉm tra API key c√≥ t·ªìn t·∫°i v√† h·ª£p l·ªá kh√¥ng
    """
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
        if not api_key:
            st.error("‚ö†Ô∏è GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh!")
            st.stop()
        return api_key
    except Exception as e:
        st.error("‚ö†Ô∏è GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh trong Streamlit Secrets!")
        st.stop()

def validate_url(url):
    """
    Ki·ªÉm tra URL c√≥ h·ª£p l·ªá kh√¥ng
    """
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

class ArticleSummarizer:
    def __init__(self):
        self.gemini_api_key = check_api_key()
        genai.configure(api_key=self.gemini_api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    async def fetch_url(self, url):
        """
        ƒê·ªçc URL b·∫•t ƒë·ªìng b·ªô s·ª≠ d·ª•ng aiohttp
        """
        try:
            async with aiohttp.ClientSession(headers=self.headers) as session:
                async with session.get(url) as response:
                    return await response.text()
        except Exception as e:
            raise Exception(f"L·ªói khi ƒë·ªçc URL {url}: {str(e)}")

    def extract_content_from_html(self, html):
        """
        Tr√≠ch xu·∫•t n·ªôi dung t·ª´ HTML s·ª≠ d·ª•ng BeautifulSoup
        """
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'iframe']):
                tag.decompose()
            
            # L·∫•y n·ªôi dung t·ª´ c√°c th·∫ª p
            paragraphs = soup.find_all('p')
            content = ' '.join([p.get_text().strip() for p in paragraphs])
            
            return content
        except Exception as e:
            raise Exception(f"L·ªói khi parse HTML: {str(e)}")

    async def extract_content_from_url(self, url):
        """
        Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL
        """
        html = await self.fetch_url(url)
        return self.extract_content_from_html(html)

    async def process_urls(self, urls):
        """
        X·ª≠ l√Ω nhi·ªÅu URLs ƒë·ªìng th·ªùi
        """
        try:
            start_time = time.time()
            
            # ƒê·ªçc n·ªôi dung t·ª´ t·∫•t c·∫£ URLs ƒë·ªìng th·ªùi
            contents = await asyncio.gather(
                *[self.extract_content_from_url(url.strip()) for url in urls]
            )
            
            # K·∫øt h·ª£p n·ªôi dung
            combined_content = "\n\n---\n\n".join(contents)
            
            print(f"Th·ªùi gian ƒë·ªçc URLs: {time.time() - start_time:.2f} gi√¢y")
            
            # X·ª≠ l√Ω v·ªõi Gemini
            result = await self.process_content(combined_content, urls)
            result['original_urls'] = urls
            
            print(f"T·ªïng th·ªùi gian x·ª≠ l√Ω: {time.time() - start_time:.2f} gi√¢y")
            
            return result
            
        except Exception as e:
            raise Exception(f"L·ªói x·ª≠ l√Ω URLs: {str(e)}")

    async def process_content(self, content, urls):
        """
        X·ª≠ l√Ω n·ªôi dung v·ªõi Gemini
        """
        try:
            # B∆∞·ªõc 1: T√≥m t·∫Øt v√† t·∫°o ti√™u ƒë·ªÅ ti·∫øng Anh
            english_prompt = f"""
            Please process this Vietnamese text:
            1. Translate to English
            2. Create a summary (500-1000 words)
            3. Generate a title that captures the main theme
            
            Format your response exactly as:
            TITLE: [your title]
            SUMMARY: [your summary]

            Text to process: {content}
            """
            
            english_response = self.model.generate_content(english_prompt)
            english_result = english_response.text
            
            # Parse k·∫øt qu·∫£ ti·∫øng Anh
            try:
                en_title = english_result.split('TITLE:')[1].split('SUMMARY:')[0].strip()
                en_summary = english_result.split('SUMMARY:')[1].strip()
                
                # Ki·ªÉm tra ƒë·ªô d√†i c·ªßa b·∫£n t√≥m t·∫Øt
                word_count = len(en_summary.split())
                
                if word_count < 500:
                    expand_prompt = f"""
                    The current summary is too short ({word_count} words). 
                    Please expand this summary to be between 500-1000 words by:
                    1. Adding more detailed analysis
                    2. Including relevant context and background information
                    3. Providing more specific examples and explanations
                    4. Elaborating on key points
                    
                    Current summary:
                    {en_summary}
                    """
                    
                    expand_response = self.model.generate_content(expand_prompt)
                    en_summary = expand_response.text
                    word_count = len(en_summary.split())  # C·∫≠p nh·∫≠t l·∫°i word_count
                
            except Exception as e:
                raise Exception(f"Kh√¥ng th·ªÉ parse k·∫øt qu·∫£ ti·∫øng Anh: {str(e)}")
            
            # B∆∞·ªõc 2: D·ªãch sang ti·∫øng Vi·ªát
            vietnamese_prompt = f"""
            Translate this English title and summary to Vietnamese.
            Format your response exactly as:
            TITLE: [Vietnamese title]
            SUMMARY: [Vietnamese summary]

            English text:
            TITLE: {en_title}
            SUMMARY: {en_summary}
            """
            
            vietnamese_response = self.model.generate_content(vietnamese_prompt)
            vietnamese_result = vietnamese_response.text
            
            # Parse k·∫øt qu·∫£ ti·∫øng Vi·ªát
            try:
                vi_title = vietnamese_result.split('TITLE:')[1].split('SUMMARY:')[0].strip()
                vi_summary = vietnamese_result.split('SUMMARY:')[1].strip()
                vi_word_count = len(vi_summary.split())  # ƒê·∫øm s·ªë t·ª´ ti·∫øng Vi·ªát
            except Exception as e:
                raise Exception(f"Kh√¥ng th·ªÉ parse k·∫øt qu·∫£ ti·∫øng Vi·ªát: {str(e)}")
            
            return {
                'title': vi_title,
                'content': vi_summary,
                'english_title': en_title,
                'english_summary': en_summary,
                'word_count': word_count,  # S·ªë t·ª´ ti·∫øng Anh
                'vi_word_count': vi_word_count,  # S·ªë t·ª´ ti·∫øng Vi·ªát
                'original_urls': urls  # Th√™m URLs g·ªëc v√†o k·∫øt qu·∫£
            }
            
        except Exception as e:
            raise Exception(f"L·ªói x·ª≠ l√Ω Gemini: {str(e)}")

async def process_and_update_ui(summarizer, urls):
    try:
        result = await summarizer.process_urls(urls)
        return result
    except Exception as e:
        raise e

def main():
    st.set_page_config(page_title="·ª®ng d·ª•ng T√≥m t·∫Øt VƒÉn b·∫£n", page_icon="üìù", layout="wide")
    
    st.title("üìù ·ª®ng d·ª•ng T√≥m t·∫Øt Nhi·ªÅu B√†i B√°o")
    st.markdown("---")

    if 'summarizer' not in st.session_state:
        st.session_state.summarizer = ArticleSummarizer()

    with st.container():
        st.subheader("üîó Nh·∫≠p URL c√°c b√†i b√°o")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            url1 = st.text_input("URL b√†i b√°o 1", key="url1")
        with col2:
            url2 = st.text_input("URL b√†i b√°o 2", key="url2")
        with col3:
            url3 = st.text_input("URL b√†i b√°o 3", key="url3")
        
        urls = [url1, url2, url3]
        
        if st.button("T√≥m t·∫Øt", type="primary"):
            if not all(urls):
                st.warning("Vui l√≤ng nh·∫≠p ƒë·ªß 3 URLs!")
                return
            
            invalid_urls = [url for url in urls if not validate_url(url)]
            if invalid_urls:
                st.error(f"C√°c URLs sau kh√¥ng h·ª£p l·ªá: {', '.join(invalid_urls)}")
                return
            
            progress_text = "ƒêang x·ª≠ l√Ω..."
            progress_bar = st.progress(0, text=progress_text)
            
            try:
                result = asyncio.run(process_and_update_ui(st.session_state.summarizer, urls))
                
                if result:
                    progress_bar.progress(100, text="Ho√†n th√†nh!")
                    st.success(f"‚úÖ T√≥m t·∫Øt th√†nh c√¥ng! (ƒê·ªô d√†i: {result['vi_word_count']} t·ª´ ti·∫øng Vi·ªát, {result['word_count']} t·ª´ ti·∫øng Anh)")
                    
                    st.markdown(f"## üìå {result['title']}")
                    st.markdown("### üìÑ B·∫£n t√≥m t·∫Øt")
                    st.write(result['content'])
                    
                    with st.expander("Xem phi√™n b·∫£n ti·∫øng Anh"):
                        st.markdown(f"### {result['english_title']}")
                        st.write(result['english_summary'])
                    
                    with st.expander("Xem URLs g·ªëc"):
                        for i, url in enumerate(result['original_urls'], 1):
                            st.markdown(f"B√†i {i}: [{url}]({url})")
                            
            except Exception as e:
                st.error(f"C√≥ l·ªói x·∫£y ra: {str(e)}")
            finally:
                progress_bar.empty()

if __name__ == "__main__":
    main()
